# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# see kafka.server.KafkaConfig for additional details and defaults

############################# General Configuration #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=<%= node[:kafka][:broker_id] %>

# The maximum size of message that the server can receive
message.max.bytes=<%= node[:kafka][:message_max_bytes] %>

# The number of network threads that the server uses for handling network requests
num.network.threads=<%= node[:kafka][:num_network_threads] %>

# The number of io threads that the server uses for carrying out network requests
num.io.threads=<%= node[:kafka][:num_io_threads] %>

# The number of queued requests allowed before blocking the network threads
queued.max.requests=<%= node[:kafka][:queued_max_requests] %>

############################# Socket Server Configuration #############################

# The port to listen and accept connections on
port=<%= node[:kafka][:port] %>

# Hostname of broker. If this is set, it will only bind to this address. If this is not set,
# it will bind to all interfaces, and publish one to ZK.
<% if node[:kafka][:host_name] && !node[:kafka][:host_name].empty? %>
host.name=<%= node[:kafka][:host_name] %>
<% else %>
#host.name=
<% end %>

# The SO_SNDBUFF buffer of the socket sever sockets
socket.send.buffer.bytes=<%= node[:kafka][:socket][:send_buffer_bytes] %>

# The SO_RCVBUFF buffer of the socket sever sockets
socket.receive.buffer.bytes=<%= node[:kafka][:socket][:receive_buffer_bytes] %>

# The maximum number of bytes in a socket request
socket.request.max.bytes=<%= node[:kafka][:socket][:request_max_bytes] %>

############################# Log Configuration #############################

# The default number of log partitions per topic
num.partitions=<%= node[:kafka][:num_partitions] %>

# The directories in which the log data is kept
log.dirs=<%= node[:kafka][:log][:dirs].join(',') %>

# The maximum size of a single log file
log.segment.bytes=<%= node[:kafka][:log][:segment_bytes] %>

# The maximum size of a single log file for some specific topic
<% if node[:kafka][:log][:segment_bytes_per_topic].any? %>
log.segment.bytes.per.topic=<%= node[:kafka][:log][:segment_bytes_per_topic].map {|k,v| [k, v].join(':')}.join(',') %>
<% else %>
#log.segment.bytes.per.topic=
<% end %>

# the maximum time before a new log segment is rolled out
log.roll.hours=<%= node[:kafka][:log][:roll_hours] %>

# the number of hours before rolling out a new log segment for some specific topic
<% if node[:kafka][:log][:roll_hours_per_topic].any? %>
log.roll.hours.per.topic=<%= node[:kafka][:log][:roll_hours_per_topic].map {|k,v| [k, v].join(':')}.join(',') %>
<% else %>
#log.roll.hours.per.topic=
<% end %>

# the number of hours to keep a log file before deleting it
log.retention.hours=<%= node[:kafka][:log][:retention_hours] %>

# the number of hours to keep a log file before deleting it for some specific topic
<% if node[:kafka][:log][:retention_hours_per_topic].any? %>
log.retention.hours.per.topic=<%= node[:kafka][:log][:retention_hours_per_topic].map {|k,v| [k, v].join(':')}.join(',') %>
<% else %>
#log.retention.hours.per.topic=
<% end %>

# the maximum size of the log before deleting it
log.retention.bytes=<%= node[:kafka][:log][:retention_bytes] %>

# the maximum size of the log for some specific topic before deleting it
<% if node[:kafka][:log][:retention_bytes_per_topic].any? %>
log.retention.bytes.per.topic=<%= node[:kafka][:log][:retention_bytes_per_topic].map {|k,v| [k, v].join(':')}.join(',') %>
<% else %>
#log.retention.bytes.per.topic=
<% end %>

# the frequency in minutes that the log cleaner checks whether any log is eligible for deletion
log.cleanup.interval.mins=<%= node[:kafka][:log][:cleanup_interval_mins] %>

# the maximum size in bytes of the offset index
log.index.size.max.bytes=<%= node[:kafka][:log][:index_size_max_bytes] %>

# the interval with which we add an entry to the offset index
log.index.interval.bytes=<%= node[:kafka][:log][:index_interval_bytes] %>

# the number of messages accumulated on a log partition before messages are flushed to disk
log.flush.interval.messages=<%= node[:kafka][:log][:flush_interval_messages] %>

# the maximum time in ms that a message in any topic is kept in memory before flushed to disk
log.flush.interval.ms=<%= node[:kafka][:log][:flush_interval_ms] %>

# the maximum time in ms that a message in selected topics is kept in memory before flushed to disk, e.g., topic1:3000,topic2:6000
<% if node[:kafka][:log][:flush_interval_ms_per_topic].any? %>
log.flush.interval.ms.per.topic=<%= node[:kafka][:log][:flush_interval_ms_per_topic].map {|k,v| [k, v].join(':')}.join(',') %>
<% else %>
#log.flush.interval.ms.per.topic=
<% end %>

# the frequency in ms that the log flusher checks whether any log needs to be flushed to disk
log.flush.scheduler.interval.ms=<%= node[:kafka][:log][:flush_scheduler_interval_ms] %>

# enable auto creation of topic on the broker
auto.create.topics.enable=<%= node[:kafka][:auto_create_topics] %>

############################# Replication Configuration #############################

# the socket timeout for controller-to-broker channels
controller.socket.timeout.ms=<%= node[:kafka][:controller][:socket_timeout_ms] %>

# the buffer size for controller-to-broker-channels
controller.message.queue.size=<%= node[:kafka][:controller][:message_queue_size] %>

# default replication factor for automatically created topics
default.replication.factor=<%= node[:kafka][:default_replication_factor] %>

# if a follower hasn't sent any fetch requests during this time, the leader will remove the follower from isr
replica.lag.time.max.ms=<%= node[:kafka][:replica][:lag_time_max_ms] %>

# If the lag in messages between a leader and a follower exceeds this number, the leader will remove the follower from isr
replica.lag.max.messages=<%= node[:kafka][:replica][:lag_max_messages] %>

# the socket timeout for network requests
replica.socket.timeout.ms=<%= node[:kafka][:replica][:socket_timeout_ms] %>

# the socket receive buffer for network requests
replica.socket.receive.buffer.bytes=<%= node[:kafka][:replica][:socket_receive_buffer_bytes] %>

# the number of bytes of messages to attempt to fetch
replica.fetch.max.bytes=<%= node[:kafka][:replica][:fetch_max_bytes] %>

# minimum bytes expected for each fetch response. If not enough bytes, wait up to replicaMaxWaitTimeMs
replica.fetch.min.bytes=<%= node[:kafka][:replica][:fetch_min_bytes] %>

# max wait time for each fetcher request issued by follower replicas
replica.fetch.wait.max.ms=<%= node[:kafka][:replica][:fetch_wait_max_ms] %>

# number of fetcher threads used to replicate messages from a source broker.
# Increasing this value can increase the degree of I/O parallelism in the follower broker.
num.replica.fetchers=<%= node[:kafka][:num_replica_fetchers] %>

# the frequency with which the high watermark is saved out to disk
replica.high.watermark.checkpoint.interval.ms=<%= node[:kafka][:replica][:high_watermark_checkpoint_interval_ms] %>

# the purge interval (in number of requests) of the fetch request purgatory
fetch.purgatory.purge.interval.requests=<%= node[:kafka][:fetch][:purgatory_purge_interval_requests] %>

# the purge interval (in number of requests) of the producer request purgatory
producer.purgatory.purge.interval.requests=<%= node[:kafka][:producer][:purgatory_purge_interval_requests] %>

############################# Controlled shutdown configuration #############################

# Controlled shutdown can fail for multiple reasons. This determines the number of retries when such failure happens
controlled.shutdown.max.retries=<%= node[:kafka][:controlled_shutdown][:max_retries] %>

# Before each retry, the system needs time to recover from the state that caused the previous failure (Controller
# fail over, replica lag etc). This config determines the amount of time to wait before retrying.
controlled.shutdown.retry.backoff.ms=<%= node[:kafka][:controlled_shutdown][:retry_backoff_ms] %>

# enable controlled shutdown of the server
controlled.shutdown.enable=<%= node[:kafka][:controlled_shutdown][:enabled] %>

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=<%= node[:kafka][:zookeeper][:connect].join(',') %>

# the max time that the client waits to establish a connection to zookeeper
zookeeper.connection.timeout.ms=<%= node[:kafka][:zookeeper][:connection_timeout_ms] %>

# zookeeper session timeout
zookeeper.session.timeout.ms=<%= node[:kafka][:zookeeper][:session_timeout_ms] %>

# how far a ZK follower can be behind a ZK leader
zookeeper.sync.time.ms=<%= node[:kafka][:zookeeper][:sync_time_ms] %>
